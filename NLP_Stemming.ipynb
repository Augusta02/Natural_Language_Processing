{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQOr9MipyYmh3MQ6QoO6an",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Augusta02/Natural_Language_Processing/blob/main/NLP_Stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "apRTBjKMK20d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR8LLWK6vO6l",
        "outputId": "0a32ca1a-d44e-473e-b2c7-28f058d88982"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Normalization\n",
        "\n",
        "This is used to change words to their common form, when the variations mean the same thing. It involves the cleaning and preprocessing of data to be in a consistent form.\n",
        "\n",
        "The process involves the removal of stop words, punctuations, stemmization, lemmization, handling capitalzation.\n",
        "\n",
        "\n",
        "# Steps for Text Normalization\n"
      ],
      "metadata": {
        "id": "CA9sYFanmtjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Normalization"
      ],
      "metadata": {
        "id": "eaVOQ__J0m28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ao8Sm_iO472A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# case normalization\n",
        "# this can be used by applying the lower()\n",
        "\n",
        "text = 'Mr Kaya, bought a New P-40N Warhawk and Kayla is going on Vacation with her friends!!!!'\n",
        "\n",
        "text_norm = text.lower()\n",
        "\n",
        "print(text_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6GawR9PoHCT",
        "outputId": "d1722261-42c7-4752-92f8-bcb8562ec919"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mr kaya, bought a new p-40n warhawk and kayla is going on vacation with her friends!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Punctuations"
      ],
      "metadata": {
        "id": "wkEWr1CS0rHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# handling punctuations\n",
        "# using the string maketrans to remove the punctuation\n",
        "# punct = 'Mr Kaya, bought a New P-40N Warhawk!!!!'\n",
        "\n",
        "punctuation = string.punctuation\n",
        "text_= text.maketrans('', '', punctuation)\n",
        "\n",
        "print(text.translate(text_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3HnyCmCo8ke",
        "outputId": "e52d00d2-0105-4571-e68b-df8a3dcfa714"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr Kaya bought a New P40N Warhawk and Kayla is going on Vacation with her friends\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Word Removal"
      ],
      "metadata": {
        "id": "TXgh5RWo0vlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop word removal\n",
        "# this is the process of removing common words\n",
        "# with little meaning to the sentence such as 'the', 'is', 'and', 'a', etc.\n",
        "# A useful technique when handling text with alot of stop words\n",
        "# This helps to reduce dimensionality  and removing unneccessary words\n",
        "# its downside is, it can lead to loss of information\n",
        "# stopwords can be used emphasize sentiment\n",
        "\n",
        "text = 'Mr Kaya bought a New P-40N Warhawk and Kayla is Driving with the Roof OPEN!!!!'\n",
        "from nltk.corpus import stopwords\n",
        "# set stopwords language\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_ = text.split()\n",
        "# loop through word_ and check if the word is a stopword  and remove them\n",
        "filter_word = [word for word in word_ if word not in stop_words]\n",
        "\n",
        "'''OUTPUT: ['Mr', 'Kaya', 'bought', 'New', 'P-40N', 'Warhawk', 'Kayla', 'Driving', 'Roof', 'OPEN!!!!'] '''\n",
        "\n",
        "# join text\n",
        "\n",
        "join_word = ' '.join(filter_word)\n",
        "print(join_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tzL-YZyrlGI",
        "outputId": "d7bcfb2f-65ea-4426-c154-e9a80cd3ef03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr Kaya bought New P-40N Warhawk Kayla Driving Roof OPEN!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemmization\n",
        "\n",
        "This is the reduction of words to their root forms, this is done by removing affixes. There are three popular algorithms used for stemmization and they are:\n",
        "- Porter Algorithm: This is the oldest algorithm,and its basic rule is to remove common affixes.\n",
        "  - Step 1: Remove -s or -es if the word ends in -ess, -ed, -ing, -y, or -ly.\n",
        "  - Step 2: If the word ends in -y, and the preceding consonant is not \"w\" or \"x\", then remove the -y and add -i.\n",
        "  - Step 3: If the word ends in -e, and the preceding vowel is not \"a\", \"i\", or \"o\", then remove the -e.\n",
        "  - Step 4: If the word ends in -l, then remove the -l unless the word ends in -ll.\n",
        "  - Step 5: If the word ends in -er, then remove the -er unless the word is \"better\", \"worse\", \"later\", or \"under\".\n",
        "  - Step 6: If the word ends in -ing, then remove the -ing unless the word ends in -inge.\n",
        "  - Step 7: If the word ends in -ed, then remove the -ed unless the word ends in -eed.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oA63J_X5016O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "p_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "S8_Qutf5MB5g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words= ['run', 'running', 'runner', 'runs', 'runners', 'ran']\n",
        "\n",
        "for word in words:\n",
        "  print(word + ' --->' + p_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDaAJQkcMRSQ",
        "outputId": "9930c502-2f28-43cd-dcc7-bb61d8612860"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run --->run\n",
            "running --->run\n",
            "runner --->runner\n",
            "runs --->run\n",
            "runners --->runner\n",
            "ran --->ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "s_stemmer= SnowballStemmer(language='english')\n",
        "\n",
        "for word in words:\n",
        "  print(word + '---->' + s_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_gU3FfUNFt4",
        "outputId": "73fe04b2-b4f1-4074-bfc8-29c1132b4431"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run---->run\n",
            "running---->run\n",
            "runner---->runner\n",
            "runs---->run\n",
            "runners---->runner\n",
            "ran---->ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snowball\n",
        "\n",
        " It is an improved Porter Algorithm, The Snowball algorithm is a recursive algorithm, which means that it breaks down a word into its stem by repeatedly applying a set of rules. The rules in the Snowball algorithm are designed to remove suffixes from words, while preserving the stem of the word."
      ],
      "metadata": {
        "id": "GJEASVSk3Vqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_words = ['generate', 'generous', 'generation', 'generously']\n",
        "\n",
        "for new in new_words:\n",
        "  print(new + '---->' + p_stemmer.stem(new))\n",
        "  print(new + '---->' + s_stemmer.stem(new))\n",
        "  print('-------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auKbAwR6NpOU",
        "outputId": "c59cbd36-42ae-4254-8ff5-efa5d50804a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate---->gener\n",
            "generate---->generat\n",
            "-------------\n",
            "generous---->gener\n",
            "generous---->generous\n",
            "-------------\n",
            "generation---->gener\n",
            "generation---->generat\n",
            "-------------\n",
            "generously---->gener\n",
            "generously---->generous\n",
            "-------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball is the newest NLP stemmization algorithm which was built using the basics of Porter Algorithms."
      ],
      "metadata": {
        "id": "caBNkbPXOXUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Lancaster Algorithm:\n",
        "\n",
        "This is a more aggressive algorithm and it tends to over-stem words. This means that it may reduce a word to its root form, even if the root form is not the most common or correct form of the word."
      ],
      "metadata": {
        "id": "UJ3sTdu53oOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ls= LancasterStemmer()"
      ],
      "metadata": {
        "id": "m-CAUbozSMEC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = ['is', 'was', 'are','be','been', 'were']\n",
        "\n",
        "for x in y:\n",
        "  print(f'{x} has the stemming of {p_stemmer.stem(x)}')\n",
        "  print(f'{x} has the stemming of {ls.stem(x)}')\n",
        "  print('-----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb9WTfT1TLIQ",
        "outputId": "996619e0-315f-4477-d0cd-eb8860ff13ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is has the stemming of is\n",
            "is has the stemming of is\n",
            "-----------------\n",
            "was has the stemming of wa\n",
            "was has the stemming of was\n",
            "-----------------\n",
            "are has the stemming of are\n",
            "are has the stemming of ar\n",
            "-----------------\n",
            "be has the stemming of be\n",
            "be has the stemming of be\n",
            "-----------------\n",
            "been has the stemming of been\n",
            "been has the stemming of been\n",
            "-----------------\n",
            "were has the stemming of were\n",
            "were has the stemming of wer\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word= ['smiling', 'lead', 'marked', 'leader', 'likely', 'city', 'pretty', 'formula', 'drama', 'hero', 'solo', 'smiles']\n",
        "\n",
        "for w in word:\n",
        "  print(f'{w} has stem  ----> porter: {p_stemmer.stem(w)}')\n",
        "  print('-----------')\n",
        "  print(f'{w} has stem ------> snowball: {s_stemmer.stem(w)}')\n",
        "  print('--------------')\n",
        "  print(f'{w} has stem --------> lancaster: {ls.stem(w)}')\n",
        "  print('-------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pVcK-IK2Vvl",
        "outputId": "2de1c5f9-0bb3-4676-9728-be13335523e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smiling has stem  ----> porter: smile\n",
            "-----------\n",
            "smiling has stem ------> snowball: smile\n",
            "--------------\n",
            "smiling has stem --------> lancaster: smil\n",
            "-------------\n",
            "lead has stem  ----> porter: lead\n",
            "-----------\n",
            "lead has stem ------> snowball: lead\n",
            "--------------\n",
            "lead has stem --------> lancaster: lead\n",
            "-------------\n",
            "marked has stem  ----> porter: mark\n",
            "-----------\n",
            "marked has stem ------> snowball: mark\n",
            "--------------\n",
            "marked has stem --------> lancaster: mark\n",
            "-------------\n",
            "leader has stem  ----> porter: leader\n",
            "-----------\n",
            "leader has stem ------> snowball: leader\n",
            "--------------\n",
            "leader has stem --------> lancaster: lead\n",
            "-------------\n",
            "likely has stem  ----> porter: like\n",
            "-----------\n",
            "likely has stem ------> snowball: like\n",
            "--------------\n",
            "likely has stem --------> lancaster: lik\n",
            "-------------\n",
            "city has stem  ----> porter: citi\n",
            "-----------\n",
            "city has stem ------> snowball: citi\n",
            "--------------\n",
            "city has stem --------> lancaster: city\n",
            "-------------\n",
            "pretty has stem  ----> porter: pretti\n",
            "-----------\n",
            "pretty has stem ------> snowball: pretti\n",
            "--------------\n",
            "pretty has stem --------> lancaster: pretty\n",
            "-------------\n",
            "formula has stem  ----> porter: formula\n",
            "-----------\n",
            "formula has stem ------> snowball: formula\n",
            "--------------\n",
            "formula has stem --------> lancaster: formul\n",
            "-------------\n",
            "drama has stem  ----> porter: drama\n",
            "-----------\n",
            "drama has stem ------> snowball: drama\n",
            "--------------\n",
            "drama has stem --------> lancaster: dram\n",
            "-------------\n",
            "hero has stem  ----> porter: hero\n",
            "-----------\n",
            "hero has stem ------> snowball: hero\n",
            "--------------\n",
            "hero has stem --------> lancaster: hero\n",
            "-------------\n",
            "solo has stem  ----> porter: solo\n",
            "-----------\n",
            "solo has stem ------> snowball: solo\n",
            "--------------\n",
            "solo has stem --------> lancaster: solo\n",
            "-------------\n",
            "smiles has stem  ----> porter: smile\n",
            "-----------\n",
            "smiles has stem ------> snowball: smile\n",
            "--------------\n",
            "smiles has stem --------> lancaster: smil\n",
            "-------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "This is the process of grouping together different inflected forms of a word so they can be analyzed as a single item. It is identified by the word's lemma or dictionary form.\n",
        "\n",
        "- it is more accurate than stems such that it accurately identifies words and group together correctly.\n",
        "- it reduces amabuiguity of nlp tasks, stems have different meaning, such as bank which can be a financial instituition or side of the river, with context or analyzed group of words together, it understand the use of the word 'bank' in the sentence.\n",
        "- It helps reduce the number of words to process"
      ],
      "metadata": {
        "id": "oxBe2X_ME48Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# spacy.load is amethod used to specify the language model\n",
        "# en_core_web_sm is a small english language model that is accurate and fast\n",
        "# which can be applied on text or code.\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "3fdyeAs1GYCx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''The u in the expression text = nlp(u'I can be a data scientist and a blockchain engineer\n",
        "in the United State of America') is a prefix that tells Python that the string is encoded in Unicode. This is necessary because spaCy uses Unicode internally,\n",
        "and it needs to know that the string is encoded in Unicode in order to process it correctly.'''\n",
        "\n",
        "text = nlp(u'I can be a data scientist and a blockchain engineer in the United State of America')\n",
        "\n",
        "for t in text:\n",
        "  print(t.text,'\\t', t.lemma, '\\t', t.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8wWTGLxJXvX",
        "outputId": "f986392c-7a33-4d3d-bc36-e6ceff10ed8a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I \t 4690420944186131903 \t I\n",
            "can \t 6635067063807956629 \t can\n",
            "be \t 10382539506755952630 \t be\n",
            "a \t 11901859001352538922 \t a\n",
            "data \t 6645506661261177361 \t data\n",
            "scientist \t 16370364435822077466 \t scientist\n",
            "and \t 2283656566040971221 \t and\n",
            "a \t 11901859001352538922 \t a\n",
            "blockchain \t 13707632201927900929 \t blockchain\n",
            "engineer \t 2945926927285067412 \t engineer\n",
            "in \t 3002984154512732771 \t in\n",
            "the \t 7425985699627899538 \t the\n",
            "United \t 13226800834791099135 \t United\n",
            "State \t 3438489356621435858 \t State\n",
            "of \t 886050111519832510 \t of\n",
            "America \t 13134984502707718284 \t America\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordNetLemmatizer\n",
        "\n",
        "This is used to find the singular forms of words."
      ],
      "metadata": {
        "id": "PXQlK__4MUeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTG0jqkwNaes",
        "outputId": "fae91d46-f407-4a54-9417-81b1ea6987d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "ml= ['cats', 'boxes', 'radii', 'cacti', 'visionaries', 'runners', 'speeches']\n",
        "\n",
        "for m in ml:\n",
        "  print(lem.lemmatize(m))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihyxx76yMdvJ",
        "outputId": "32c9901a-83db-4a2a-faf9-7a2f5a47b4c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "box\n",
            "radius\n",
            "cactus\n",
            "visionary\n",
            "runner\n",
            "speech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indicating the part of speech of the word\n",
        "# mennatize can return the word and in its part of speech\n",
        "\n",
        "print(lem.lemmatize('beauty', 'n'))\n",
        "print(lem.lemmatize('beauty', 'v'))\n",
        "# print(lem.lemmatize('beautiful', 'adj'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOFxwK0ROd5S",
        "outputId": "1997a5f0-c015-42a0-8d66-e955092a3998"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beauty\n",
            "beauty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 4 of Language Processing\n",
        "\n",
        "# Syntatic Analysis\n",
        "\n",
        "Syntax in language means the arrangement of words. Syntatic analysis is the processing of words as a cluster. The goal of syntathic analysis is to determine the grammatical structure of the sentence. Therefore, we would learn the how words depend on each other, word class, word order, grammatical relations and consistuency parser.\n",
        "\n",
        "\n",
        "\n",
        "## Word class:\n",
        "> In natural language processing (NLP), a word class is a set of words that have similar grammatical properties. They are essential for NLP because they help to determine the meaning of words and sentences.\n",
        " - These words behave alike eg, dogs and cats which are Nouns\n",
        " - perform similar functions eg, walk and run which are action words Verbs\n",
        " - they undergo similar transformation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Part of Speech\n",
        "\n",
        "> These are the traditonal parts of speech:\n",
        "- Nouns: Words that refer to people, places, things, or ideas.\n",
        "- Verbs: Words that describe actions or states of being.\n",
        "- Adjectives: Words that describe nouns.\n",
        "- Adverbs: Words that modify verbs, adjectives, or other adverbs.\n",
        "- Determiners: Words that precede nouns to indicate quantity, number, or definiteness.\n",
        "- Pronouns: Words that take the place of nouns.\n",
        "- Prepositions: Words that show the relationship between a noun or pronoun and another word in a sentence.\n",
        "- Conjunctions: Words that join words, phrases, or clauses together.\n",
        "- Interjections: Words that express emotions or sudden thoughts.\n",
        "\n",
        "They are called part of speech,POS, lexical category, word classes, morphological classes, lexical tag.\n",
        "\n",
        "\n",
        "# POS Tagging: This is the process of assigning part of speech of each word in a corpus.\n",
        "\n",
        "Importance of POS Tagging\n",
        "- It helps understand how words can be joined together to create grammatical correct sentences.\n",
        "- It helps in stemming, if part of speech of the word is correctly identified, it would provide a logical root of the word.\n",
        "- it helps in predicting follow up words, such as Possissive pronouns (my, your) are followed by nouns while Personal pronouns are followed by verbs.\n",
        "- It helps in automatic disambiguation of words, such that when it identifies the part of speech of the word, it understand the context of the sentence. Eg, I went to the bank. The word bank could mean a 'financial institution' or 'the side of a river' which are Nouns. This helps to narrow down the word.\n",
        "\n",
        "\n",
        "## Open and Closed Classed Words\n",
        "\n",
        "Parts of speech are divided into two;\n",
        "- Open Class: these are words that accept addition of other words to them through morphological processes such as compounding or affixes, eg faxed\n",
        "- Close Class: these are words that do not accept addition of words to them. eg pronouns, auxilary verbs, or words that are function words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mj2ME6ldpH1S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVBUC1sqOhDZ"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}