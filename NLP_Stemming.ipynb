{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhsJYkxxM1E9g/a767M9A6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Augusta02/Natural_Language_Processing/blob/main/NLP_Stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apRTBjKMK20d"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "p_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "S8_Qutf5MB5g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words= ['run', 'running', 'runner', 'runs', 'runners', 'ran']\n",
        "\n",
        "for word in words:\n",
        "  print(word + ' --->' + p_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDaAJQkcMRSQ",
        "outputId": "43ba9db6-8a0d-4b0b-eeb5-ad51022fec04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run --->run\n",
            "running --->run\n",
            "runner --->runner\n",
            "runs --->run\n",
            "runners --->runner\n",
            "ran --->ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "s_stemmer= SnowballStemmer(language='english')\n",
        "\n",
        "for word in words:\n",
        "  print(word + '---->' + s_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_gU3FfUNFt4",
        "outputId": "27757253-679f-405b-ead1-a57940f82ea1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run---->run\n",
            "running---->run\n",
            "runner---->runner\n",
            "runs---->run\n",
            "runners---->runner\n",
            "ran---->ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_words = ['generate', 'generous', 'generation', 'generously']\n",
        "\n",
        "for new in new_words:\n",
        "  print(new + '---->' + p_stemmer.stem(new))\n",
        "  print(new + '---->' + s_stemmer.stem(new))\n",
        "  print('-------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auKbAwR6NpOU",
        "outputId": "8ca71209-1d71-4909-aefc-7000622ec2a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate---->gener\n",
            "generate---->generat\n",
            "-------------\n",
            "generous---->gener\n",
            "generous---->generous\n",
            "-------------\n",
            "generation---->gener\n",
            "generation---->generat\n",
            "-------------\n",
            "generously---->gener\n",
            "generously---->generous\n",
            "-------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snowball is more lenient than Porters algorithm."
      ],
      "metadata": {
        "id": "caBNkbPXOXUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ls= LancasterStemmer()"
      ],
      "metadata": {
        "id": "m-CAUbozSMEC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = ['is', 'was', 'are','be','been', 'were']\n",
        "\n",
        "for x in y:\n",
        "  print(f'{x} has the stemming of {p_stemmer.stem(x)}')\n",
        "  print(f'{x} has the stemming of {ls.stem(x)}')\n",
        "  print('-----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb9WTfT1TLIQ",
        "outputId": "cd8eaf10-9f9c-4eea-bc97-46c849f5d35c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is has the stemming of is\n",
            "is has the stemming of is\n",
            "-----------------\n",
            "was has the stemming of wa\n",
            "was has the stemming of was\n",
            "-----------------\n",
            "are has the stemming of are\n",
            "are has the stemming of ar\n",
            "-----------------\n",
            "be has the stemming of be\n",
            "be has the stemming of be\n",
            "-----------------\n",
            "been has the stemming of been\n",
            "been has the stemming of been\n",
            "-----------------\n",
            "were has the stemming of were\n",
            "were has the stemming of wer\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word= ['smiling', 'lead', 'marked', 'leader', 'likely', 'city', 'pretty', 'formula', 'drama', 'hero', 'solo', 'smiles']\n",
        "\n",
        "for w in word:\n",
        "  print(f'{w} has stem  ----> porter: {p_stemmer.stem(w)}')\n",
        "  print('-----------')\n",
        "  print(f'{w} has stem ------> snowball: {s_stemmer.stem(w)}')\n",
        "  print('--------------')\n",
        "  print(f'{w} has stem --------> lancaster: {ls.stem(w)}')\n",
        "  print('-------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pVcK-IK2Vvl",
        "outputId": "9bde0c74-b077-4b68-e12b-d608c9aeeeca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smiling has stem  ----> porter: smile\n",
            "-----------\n",
            "smiling has stem ------> snowball: smile\n",
            "--------------\n",
            "smiling has stem --------> lancaster: smil\n",
            "-------------\n",
            "lead has stem  ----> porter: lead\n",
            "-----------\n",
            "lead has stem ------> snowball: lead\n",
            "--------------\n",
            "lead has stem --------> lancaster: lead\n",
            "-------------\n",
            "cardano has stem  ----> porter: cardano\n",
            "-----------\n",
            "cardano has stem ------> snowball: cardano\n",
            "--------------\n",
            "cardano has stem --------> lancaster: cardano\n",
            "-------------\n",
            "leader has stem  ----> porter: leader\n",
            "-----------\n",
            "leader has stem ------> snowball: leader\n",
            "--------------\n",
            "leader has stem --------> lancaster: lead\n",
            "-------------\n",
            "likely has stem  ----> porter: like\n",
            "-----------\n",
            "likely has stem ------> snowball: like\n",
            "--------------\n",
            "likely has stem --------> lancaster: lik\n",
            "-------------\n",
            "city has stem  ----> porter: citi\n",
            "-----------\n",
            "city has stem ------> snowball: citi\n",
            "--------------\n",
            "city has stem --------> lancaster: city\n",
            "-------------\n",
            "pretty has stem  ----> porter: pretti\n",
            "-----------\n",
            "pretty has stem ------> snowball: pretti\n",
            "--------------\n",
            "pretty has stem --------> lancaster: pretty\n",
            "-------------\n",
            "formula has stem  ----> porter: formula\n",
            "-----------\n",
            "formula has stem ------> snowball: formula\n",
            "--------------\n",
            "formula has stem --------> lancaster: formul\n",
            "-------------\n",
            "drama has stem  ----> porter: drama\n",
            "-----------\n",
            "drama has stem ------> snowball: drama\n",
            "--------------\n",
            "drama has stem --------> lancaster: dram\n",
            "-------------\n",
            "hero has stem  ----> porter: hero\n",
            "-----------\n",
            "hero has stem ------> snowball: hero\n",
            "--------------\n",
            "hero has stem --------> lancaster: hero\n",
            "-------------\n",
            "solo has stem  ----> porter: solo\n",
            "-----------\n",
            "solo has stem ------> snowball: solo\n",
            "--------------\n",
            "solo has stem --------> lancaster: solo\n",
            "-------------\n",
            "smiles has stem  ----> porter: smile\n",
            "-----------\n",
            "smiles has stem ------> snowball: smile\n",
            "--------------\n",
            "smiles has stem --------> lancaster: smil\n",
            "-------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "This is the process of grouping together different inflected forms of a word so they can be analyzed as a single item. It is identified by the word's lemma or dictionary form.\n",
        "\n",
        "- it is more accurate than stems such that it accurately identifies words and group together correctly.\n",
        "- it reduces amabuiguity of nlp tasks, stems have different meaning, such as bank which can be a financial instituition or side of the river, with context or analyzed group of words together, it understand the use of the word 'bank' in the sentence.\n",
        "- It helps reduce the number of words to process"
      ],
      "metadata": {
        "id": "oxBe2X_ME48Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# spacy.load is amethod used to specify the language model\n",
        "# en_core_web_sm is a small english language model that is accurate and fast\n",
        "# which can be applied on text or code.\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "3fdyeAs1GYCx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''The u in the expression text = nlp(u'I can be a data scientist and a blockchain engineer\n",
        "in the United State of America') is a prefix that tells Python that the string is encoded in Unicode. This is necessary because spaCy uses Unicode internally,\n",
        "and it needs to know that the string is encoded in Unicode in order to process it correctly.'''\n",
        "\n",
        "text = nlp(u'I can be a data scientist and a blockchain engineer in the United State of America')\n",
        "\n",
        "for t in text:\n",
        "  print(t.text, '\\t', t.pos_, '\\t', t.lemma, 't', t.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8wWTGLxJXvX",
        "outputId": "cdab26e5-82f8-4289-d47f-963b045ed8a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I \t PRON \t 4690420944186131903 t I\n",
            "can \t AUX \t 6635067063807956629 t can\n",
            "be \t AUX \t 10382539506755952630 t be\n",
            "a \t DET \t 11901859001352538922 t a\n",
            "data \t NOUN \t 6645506661261177361 t data\n",
            "scientist \t NOUN \t 16370364435822077466 t scientist\n",
            "and \t CCONJ \t 2283656566040971221 t and\n",
            "a \t DET \t 11901859001352538922 t a\n",
            "blockchain \t ADJ \t 13707632201927900929 t blockchain\n",
            "engineer \t NOUN \t 2945926927285067412 t engineer\n",
            "in \t ADP \t 3002984154512732771 t in\n",
            "the \t DET \t 7425985699627899538 t the\n",
            "United \t PROPN \t 13226800834791099135 t United\n",
            "State \t PROPN \t 3438489356621435858 t State\n",
            "of \t ADP \t 886050111519832510 t of\n",
            "America \t PROPN \t 13134984502707718284 t America\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordNetLemmatizer\n",
        "\n",
        "This is used to find the singular forms of words."
      ],
      "metadata": {
        "id": "PXQlK__4MUeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTG0jqkwNaes",
        "outputId": "7b277e6e-ee44-4a66-8bb7-772fe545b687"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "ml= ['cats', 'boxes', 'radii', 'cacti', 'visionaries', 'runners', 'speeches']\n",
        "\n",
        "for m in ml:\n",
        "  print(lem.lemmatize(m))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihyxx76yMdvJ",
        "outputId": "db2684a6-73d2-44d6-8ef3-1dc34c3c1b08"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "box\n",
            "radius\n",
            "cactus\n",
            "visionary\n",
            "runner\n",
            "speech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indicating the part of speech of the word\n",
        "# mennatize can return the word and in its part of speech\n",
        "\n",
        "print(lem.lemmatize('beauty', 'n'))\n",
        "print(lem.lemmatize('beauty', 'v'))\n",
        "# print(lem.lemmatize('beautiful', 'adj'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOFxwK0ROd5S",
        "outputId": "e9a746af-22ed-4434-a0ff-aeddd071f68b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beauty\n",
            "beauty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVBUC1sqOhDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}